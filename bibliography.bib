% Encoding: UTF-8

@InProceedings{Pointnet2017,
  author    = {R. Q. {Charles} and H. {Su} and M. {Kaichun} and L. J. {Guibas}},
  title     = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}},
  booktitle = {{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year      = {2017},
  pages     = {77-85},
  bibdate   = {Sat Dec 7 10:05:42 MST 2003},
  coden     = {LNCSD9},
  issn      = {0302-2345},
  journal   = {arXiv preprint arXiv:1612.00593},
}

@InProceedings{Pointnet++2017,
  author    = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  title     = {{PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}},
  booktitle = {{Proceedings of the 31st International Conference on Neural Information Processing Systems}},
  year      = {2017},
  pages     = {{5105-5114}},
}

@InProceedings{VOTE3DEEP2017,
  author    = {M. {Engelcke} and D. {Rao} and D. Z. {Wang} and C. H. {Tong} and I. {Posner}},
  title     = {{Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks}},
  booktitle = {{2017 IEEE International Conference on Robotics and Automation (ICRA)}},
  year      = {2017},
  pages     = {{1355-1361}},
}

@InProceedings{KITTI2012,
  author    = {A Geiger and P Lenz and R Urtasun},
  title     = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  booktitle = {{Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year      = {2012},
}

@InProceedings{MV3D2017,
  author    = {X. {Chen} and H. {Ma} and J. {Wan} and B. {Li} and T. {Xia}},
  title     = {{Multi-view 3D Object Detection Network for Autonomous Driving}},
  booktitle = {{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year      = {2017},
  pages     = {{6526-6534}},
}

@InProceedings{AVOD2018,
  author    = {J. {Ku} and M. {Mozifian} and J. {Lee} and A. {Harakeh} and S. L. {Waslander}},
  title     = {{Joint 3D Proposal Generation and Object Detection from View Aggregation}},
  booktitle = {{2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
  year      = {2018},
}

@InProceedings{FPointnet2018,
  author    = {C. R. {Qi} and W. {Liu} and C. {Wu} and H. {Su} and L. J. {Guibas}},
  title     = {{Frustum PointNets for 3D Object Detection from RGB-D Data}},
  booktitle = {{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
  year      = {2018},
}

@Article{FrustumConvnet2019,
  author  = {Zhixin Wang and Kui Jia},
  title   = {{Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection}},
  journal = {{ArXiv},},
  year    = {{2019}},
}

@InProceedings{P.CaoandH.ChenandY.ZhangandG.Wang2019,
  author    = {P. {Cao} and H. {Chen} and Y. {Zhang} and G. {Wang}},
  title     = {{Multi-View Frustum Pointnet for Object Detection in Autonomous Driving}},
  booktitle = {{2019 IEEE International Conference on Image Processing (ICIP)}},
  year      = {2019},
}

@Misc{Lyft2019,
  author       = {Kesten, R. and Usman, M. and Houston, J. and Pandya, T. and Nadhamuni, K. and Ferreira, A. and Yuan, M. and Low, B. and Jain, A. and Ondruska, P. and Omari, S. and Shah, S. and Kulkarni, A. and Kazakova, A. and Tao, C. and Platinsky, L. and Jiang, W. and Shet, V.},
  title        = {{Lyft Level 5 AV Dataset 2019}},
  howpublished = {{online}},
  year         = {2019},
  url          = {https://level5.lyft.com/dataset/},
}

@Misc{aev2019,
  author       = {Jacob Geyer and Yohannes Kassahun and Mentar Mahmudi and Xavier Ricou and Rupesh Durgesh and Andrew S. Chung and Lorenz House Forest and Viet Hoang Pham and Maximilian Muhlegg and Sebastian Dorn and Tiffany Fernandez and Martin Jänicke and Sudesh Mirashi and Chiragkumar Savani and Martin Storm and Oleksandr Vorobiov and Peter Schuberth},
  title        = {{A2D2: AEV Autonomous Driving Dataset}},
  howpublished = {{online}},
  year         = {2019},
  url          = {http://www.a2d2.audi},
}

@Misc{Waymo2019,
  title        = {{Waymo Open Dataset: An autonomous driving dataset}},
  howpublished = {{\url{https://www.waymo.com/open}}},
  year         = {2019},
  note         = {{Accessed: 2019-09-15}},
}

@InProceedings{shi2019pointrcnn,
  author    = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  title     = {{Pointrcnn: 3d object proposal generation and detection from point cloud}},
  booktitle = {{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}},
  year      = {2019},
  pages     = {{770--779}},
}

@Article{Chen2019FastPR,
  author  = {Yi Chen and S Liu and Xi Shen and J Jia},
  title   = {{Fast Point R-CNN}},
  journal = {{ArXiv}},
  year    = {2019},
}

@Article{tran2016edward,
  author  = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B and Rudolph, Maja and Liang, Dawen and Blei, David M},
  title   = {{Edward: A library for probabilistic modeling, inference, and criticism}},
  journal = {{arXiv preprint arXiv:1610.09787}},
  year    = {2016},
}

@Article{shridhar2018uncertainty,
  author  = {{S. Kumar, L. Felix and L. Marcus}},
  title   = {{Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference}},
  journal = {{arXiv preprint arXiv:1806.05978},},
  year    = {2019},
}

@Article{sae2014taxonomy,
  author  = {{SAE On-Road Automated Vehicle Standards Committee}},
  title   = {{Taxonomy and definitions for terms related to on-road motor vehicle automated driving systems}},
  journal = {{SAE Standard J}},
  year    = {2018},
  volume  = {{3016}},
  pages   = {{4}},
}

@Article{Leonard1992,
  author   = {J.A. Leonard and M.A. Kramer and L.H. Ungar},
  title    = {A neural network architecture that computes its own reliability},
  journal  = {Computers \& Chemical Engineering},
  year     = {1992},
  volume   = {16},
  number   = {9},
  pages    = {819 - 835},
  issn     = {0098-1354},
  note     = {An International Journal of Computer Applications in Chemical Engineering},
  abstract = {Artificial neural networks (ANNs) have been used to construct empirical nonlinear models of process data. Because network models are not based on physical theory and contain nonlinearities, their predictions are suspect when extrapolating beyond the range of the original training data. With multiple correlated inputs, it is difficult to recognize when the network is extrapolating. Furthermore, due to non-uniform distribution of the training examples and noise over the domain, the network may have local areas of poor fit even when not extrapolating. Standard measures of network performance give no indication of regions of locally poor fit or possible errors due to extrapolation. This paper introduces the “validity index network” (VI-net), an extension of radial basis function networks (RBFN), that calculates the reliability and the confidence of its output and indicates local regions of poor fit and extrapolation. Because RBFNs use a composition of local fits to the data, they are readily adapted to predict local fitting accuracy. The VI-net can also detect novel input patterns in classification problems, provided that the inputs to the classifier are real values. The reliability measures of the VI-net are implemented as additional output nodes of the underlying RBFN. Weights associated with the reliability nodes are given analytically based on training statistics from the fitting of the target function, and thus the reliability measures can be added to a standard RBFN with no additional training effort.},
}

@Article{Kuleshov2018,
  author        = {Volodymyr Kuleshov and Nathan Fenner and Stefano Ermon},
  title         = {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1807.00263},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1807-00263.bib},
  eprint        = {1807.00263},
  timestamp     = {Mon, 13 Aug 2018 16:47:00 +0200},
}

@InCollection{Sensoy2018,
  author    = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
  title     = {Evidential Deep Learning to Quantify Classification Uncertainty},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {3179--3189},
}

@Article{Liang2017,
  author        = {Shiyu Liang and Yixuan Li and R. Srikant},
  title         = {Principled Detection of Out-of-Distribution Examples in Neural Networks},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1706.02690},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/LiangLS17.bib},
  eprint        = {1706.02690},
  timestamp     = {Thu, 22 Aug 2019 13:48:06 +0200},
}

@InProceedings{Gal2016,
  author    = {Yarin Gal and Zoubin Ghahramani},
  title     = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1050--1059},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
  file      = {gal16.pdf:http\://proceedings.mlr.press/v48/gal16.pdf:PDF},
}

@InProceedings{Schulam2019,
  author    = {Schulam, Peter and Saria, Suchi},
  title     = {Can You Trust This Prediction? Auditing Pointwise Reliability After Learning},
  booktitle = {Proceedings of Machine Learning Research},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1022--1031},
  month     = {16--18 Apr},
  publisher = {PMLR},
  abstract  = {To use machine learning in high stakes applications (e.g. medicine), we need tools for building confidence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been fit on different training data. The algorithm uses the gradient and Hessian of the model’s loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic backpropagation, and deep ensembles, but does not depend on specific algorithms at train-time like these methods do.},
  file      = {schulam19a.pdf:http\://proceedings.mlr.press/v89/schulam19a/schulam19a.pdf:PDF},
}

@InCollection{Malinin2018,
  author    = {Malinin, Andrey and Gales, Mark},
  title     = {Predictive Uncertainty Estimation via Prior Networks},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {7047--7058},
}

@InCollection{Lakshminarayanan2017,
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  title     = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {6402--6413},
}

@InProceedings{Blundell2015,
  author    = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  title     = {Weight Uncertainty in Neural Networks},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  year      = {2015},
  series    = {ICML’15},
  pages     = {1613–1622},
  publisher = {JMLR.org},
  location  = {Lille, France},
  numpages  = {10},
}

@InCollection{Kendall2017,
  author    = {Kendall, Alex and Gal, Yarin},
  title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {5574--5584},
}

@InProceedings{Maqueda2018,
  author    = {{Maqueda, Ana I. and Loquercio, Antonio and Gallego, Guillermo and García, Narciso and Scaramuzza, Davide}},
  title     = {{Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars}},
  booktitle = {{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR,2018)}},
  year      = {2018},
}

@InProceedings{KimJinkyuandCannyJohn2017,
  author    = {{Kim, Jinkyu and Canny, John},},
  title     = {{Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention}},
  booktitle = {{The IEEE International Conference on Computer Vision (ICCV)},},
  year      = {2017},
}

@InProceedings{Henne2020,
  author    = {Maximilian Henne and Adrian Schwaiger and Karsten Roscher and Gereon Weiss},
  title     = {Benchmarking Uncertainty Estimation Methods for Deep Learning With Safety-Related Metrics},
  booktitle = {Proceedings of the Workshop on Artificial Intelligence Safety, co-located with 34th {AAAI} Conference on Artificial Intelligence, SafeAI@AAAI 2020, New York City, NY, USA, February 7, 2020},
  year      = {2020},
  editor    = {Hu{\'{a}}scar Espinoza and Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo and Xin Cynthia Chen and Se{\'{a}}n S. {\'{O}}h{\'{E}}igeartaigh and Xiaowei Huang and Mauricio Castillo{-}Effen and Richard Mallah and John McDermid},
  volume    = {2560},
  series    = {{CEUR} Workshop Proceedings},
  pages     = {83--90},
  publisher = {CEUR-WS.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/HenneSRW20.bib},
  timestamp = {Fri, 06 Mar 2020 12:47:03 +0100},
}

@InProceedings{Chen2017,
  author    = {Z. {Chen} and X. {Huang}},
  title     = {End-to-end learning for lane keeping of self-driving cars},
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2017},
  pages     = {1856-1860},
  month     = {June},
  abstract  = {Lane keeping is an important feature for self-driving cars. This paper presents an end-to-end learning approach to obtain the proper steering angle to maintain the car in the lane. The convolutional neural network (CNN) model takes raw image frames as input and outputs the steering angles accordingly. The model is trained and evaluated using the comma.ai dataset, which contains the front view image frames and the steering angle data captured when driving on the road. Unlike the traditional approach that manually decomposes the autonomous driving problem into technical components such as lane detection, path planning and steering control, the end-to-end model can directly steer the vehicle from the front view camera data after training. It learns how to keep in lane from human driving data. Further discussion of this end-to-end approach and its limitation are also provided.},
  keywords  = {computer vision;image capture;intelligent transportation systems;neural nets;steering systems;end-to-end learning;self-driving car lane keeping;convolutional neural network model;CNN;raw image frames;comma.ai dataset;steering angle data capture;autonomous driving problem;human driving data;Data models;Roads;Training;Computational modeling;Cameras;Autonomous automobiles;Computer architecture},
}

@InProceedings{Rao2018,
  author    = {Rao, Qing and Frtunikj, Jelena},
  title     = {Deep Learning for Self-Driving Cars: Chances and Challenges},
  booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
  year      = {2018},
  series    = {SEFAIS ’18},
  pages     = {35–38},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  isbn      = {9781450357395},
  keywords  = {deep learning, automotive, functional safety},
  location  = {Gothenburg, Sweden},
  numpages  = {4},
}

@Article{Estienne2020,
  author   = {Estienne, Théo and Lerousseau, Marvin and Vakalopoulou, Maria and Alvarez Andres, Emilie and Battistella, Enzo and Carré, Alexandre and Chandra, Siddhartha and Christodoulidis, Stergios and Sahasrabudhe, Mihir and Sun, Roger and Robert, Charlotte and Talbot, Hugues and Paragios, Nikos and Deutsch, Eric},
  title    = {Deep Learning-Based Concurrent Brain Registration and Tumor Segmentation},
  journal  = {Frontiers in Computational Neuroscience},
  year     = {2020},
  volume   = {14},
  pages    = {17},
  issn     = {1662-5188},
  abstract = {Image registration and segmentation are the two most studied problems in medical image analysis. Deep learning algorithms have recently gained a lot of attention due to their success and state-of-the-art results in variety of problems and communities. In this paper, we propose a novel, efficient, and multi-task algorithm that addresses the problems of image registration and brain tumor segmentation jointly. Our method exploits the dependencies between these tasks through a natural coupling of their interdependencies during inference. In particular, the similarity constraints are relaxed within the tumor regions using an efficient and relatively simple formulation. We evaluated the performance of our formulation both quantitatively and qualitatively for registration and segmentation problems on two publicly available datasets (BraTS 2018 and OASIS 3), reporting competitive results with other recent state-of-the-art methods. Moreover, our proposed framework reports significant amelioration (p < 0.005) for the registration performance inside the tumor locations, providing a generic method that does not need any predefined conditions (e.g., absence of abnormalities) about the volumes to be registered. Our implementation is publicly available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/TheoEst/joint_registration_tumor_segmentation" xmlns:xlink="http://www.w3.org/1999/xlink">https://github.com/TheoEst/joint_registration_tumor_segmentation</ext-link>.},
}

@Article{STandel2019,
  author                 = {S Tandel, Gopal and Biswas, Mainak and G Kakde, Omprakash and Tiwari, Ashish and S Suri, Harman and Turk, Monica and Laird, John R. and Asare, Christopher K. and A Ankrah, Annabel and N Khanna, N. and K Madhusudhan, B. and Saba, Luca and Suri, Jasjit S.},
  title                  = {A Review on a Deep Learning Perspective in Brain Cancer Classification.},
  journal                = {Cancers},
  year                   = {2019},
  volume                 = {11},
  month                  = {Jan},
  abstract               = {A World Health Organization (WHO) Feb 2018 report has recently shown that mortality rate due to brain or central nervous system (CNS) cancer is the highest in the Asian continent. It is of critical importance that cancer be detected earlier so that many of these lives can be saved. Cancer grading is an important aspect for targeted therapy. As cancer diagnosis is highly invasive, time consuming and expensive, there is an immediate requirement to develop a non-invasive, cost-effective and efficient tools for brain cancer characterization and grade estimation. Brain scans using magnetic resonance imaging (MRI), computed tomography (CT), as well as other imaging modalities, are fast and safer methods for tumor detection. In this paper, we tried to summarize the pathophysiology of brain cancer, imaging modalities of brain cancer and automatic computer assisted methods for brain cancer characterization in a machine and deep learning paradigm. Another objective of this paper is to find the current issues in existing engineering methods and also project a future paradigm. Further, we have highlighted the relationship between brain cancer and other brain disorders like stroke, Alzheimer's, Parkinson's, and Wilson's disease, leukoriaosis, and other neurological disorders in the context of machine learning and the deep learning paradigm.},
  article-doi            = {10.3390/cancers11010111},
  article-pii            = {cancers-11-00111},
  electronic-issn        = {2072-6694},
  electronic-publication = {20190118},
  history                = {2019/01/24 06:01 [medline]},
  issue                  = {1},
  keywords               = {*brain, *cancer, *deep learning, *extreme learning, *imaging, *machine learning, *neurological disorders, *pathophysiology},
  language               = {eng},
  linking-issn           = {2072-6694},
  location-id            = {111},
  nlm-unique-id          = {101526829},
  owner                  = {NLM},
  print-issn             = {2072-6694},
  publication-status     = {epublish},
  revised                = {20200225},
  source                 = {Cancers (Basel). 2019 Jan 18;11(1):111. doi: 10.3390/cancers11010111.},
  status                 = {PubMed-not-MEDLINE},
  termowner              = {NOTNLM},
  title-abbreviation     = {Cancers (Basel)},
}

@InProceedings{ElKaderIsselmou2019,
  author    = {El Kader Isselmou, Abd and Xu, Guizhi and Zhang, Shuai and Saminu, Sani and Javaid, Imran},
  title     = {Deep Learning Algorithm for Brain Tumor Detection and Analysis Using MR Brain Images},
  booktitle = {Proceedings of the 2019 International Conference on Intelligent Medicine and Health},
  year      = {2019},
  series    = {ICIMH 2019},
  pages     = {28–32},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  isbn      = {9781450372862},
  keywords  = {ndice, Tumor Detection, accuracy, MRI, Deep Learning, sensitivity},
  location  = {Ningbo, China},
  numpages  = {5},
}

@InProceedings{Abdalla2018,
  author    = {H. E. M. {Abdalla} and M. Y. {Esmail}},
  title     = {Brain Tumor Detection by using Artificial Neural Network},
  booktitle = {2018 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)},
  year      = {2018},
  pages     = {1-6},
  month     = {Aug},
  abstract  = {Brain tumor is one of the most dangerous diseases which require early and accurately detection methods, now most detection and diagnosis methods depend on decision of neurospecialists, and radiologist for image evaluation which possible to human errors and time consuming. This study reviews and describe the processes and techniques used in detection brain tumor based on magnetic resonance imaging (MRI) and artificial neural networks (ANN) techniques, Which executed in the different steps of Computer Aided Detection System (CAD) after collected the image data (MRI); first stage is pre-processing and post-processing of MRI images to enhancement it and make it more suitable to analysis then used threshold to segment the MRI images by applied mean gray level method. In the second stage was used the statistical feature analysis to extract features from images; the features computed from equations of Haralick's features based on the spatial gray level dependency matrix (SGLD) of images. Then selected the suitable and best features to detect the tumor localization. In the third stage the artificial neural networks were designed; the feedforward back propagation neural network with supervised learning were apply as automatic method to classify the images under investigation into tumor or none tumor. the network performances was evaluated successfully tested and achieved the best results with accuracy of 99%, and sensitivity 97.9%.},
  keywords  = {backpropagation;biomedical MRI;brain;diseases;feature extraction;feedforward neural nets;image classification;image enhancement;image segmentation;learning (artificial intelligence);medical image processing;tumours;feedforward backpropagation neural network;diagnosis methods;detection methods;dangerous diseases;brain tumor detection;automatic method;tumor localization;spatial gray level dependency matrix;Haralick's features;statistical feature analysis;MRI images;image data;Computer Aided Detection System;artificial neural network;magnetic resonance imaging;image evaluation;Tumors;Feature extraction;Magnetic resonance imaging;Entropy;Image segmentation;Artificial neural networks;Correlation;Brain tumor;MRI image;Texture Analysis;Haralick Texure features;Artificial Neural Networks.},
}

@Article{Bojarski2016,
  author  = {Bojarski, Mariusz and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  title   = {End to End Learning for Self-Driving Cars},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1604.07316},
}

@InProceedings{ChengchengNing2017,
  author    = {{Chengcheng Ning} and {Huajun Zhou} and {Yan Song} and {Jinhui Tang}},
  title     = {Inception Single Shot MultiBox Detector for object detection},
  booktitle = {2017 IEEE International Conference on Multimedia Expo Workshops (ICMEW)},
  year      = {2017},
  pages     = {549-554},
}

@InProceedings{Redmon2016,
  author    = {J. {Redmon} and S. {Divvala} and R. {Girshick} and A. {Farhadi}},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {779-788},
  month     = {June},
  abstract  = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  issn      = {1063-6919},
  keywords  = {image classification;image representation;neural nets;object detection;you only look once;unified real-time object detection;object classifiers;bounding boxes;class probabilities;neural network;detection pipeline;detection performance;YOLO model;object representation;DPM;R-CNN;natural images;Computer architecture;Microprocessors;Object detection;Training;Real-time systems;Neural networks;Pipelines},
}

@Misc{Mulshine2015,
  author       = {{Molly Mulshine}},
  title        = {{A major flaw in Google’s algorithm allegedly tagged two black people’s faces with the word ’gorillas’}},
  howpublished = {{Online}},
  year         = {2015},
  note         = {(visited on 08/13/2020)},
  url          = {https://www.businessinsider.com/google-tags-black-people-as-gorillas-2015-7},
}

@Misc{SLJCW2018,
  author       = {{Sam Levin and Julia C Wong}},
  title        = {{Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian}},
  howpublished = {Online},
  year         = {2018},
  note         = {visited on 08/12/2020},
  doi          = {https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizonatempe},
}

@Article{Ren2017,
  author  = {S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}},
  title   = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2017},
  volume  = {39},
  number  = {6},
  pages   = {1137-1149},
}

@Article{Arnold2019,
  author   = {E. {Arnold} and O. Y. {Al-Jarrah} and M. {Dianati} and S. {Fallah} and D. {Oxtoby} and A. {Mouzakitis}},
  title    = {A Survey on 3D Object Detection Methods for Autonomous Driving Applications},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2019},
  volume   = {20},
  number   = {10},
  pages    = {3782-3795},
  month    = {Oct},
  issn     = {1558-0016},
  abstract = {An autonomous vehicle (AV) requires an accurate perception of its surrounding environment to operate reliably. The perception system of an AV, which normally employs machine learning (e.g., deep learning), transforms sensory data into semantic information that enables autonomous driving. Object detection is a fundamental function of this perception system, which has been tackled by several works, most of them using 2D detection methods. However, the 2D methods do not provide depth information, which is required for driving tasks, such as path planning, collision avoidance, and so on. Alternatively, the 3D object detection methods introduce a third dimension that reveals more detailed object's size and location information. Nonetheless, the detection accuracy of such methods needs to be improved. To the best of our knowledge, this is the first survey on 3D object detection methods used for autonomous driving applications. This paper presents an overview of 3D object detection methods and prevalently used sensors and datasets in AVs. It then discusses and categorizes the recent works based on sensors modalities into monocular, point cloud-based, and fusion methods. We then summarize the results of the surveyed works and identify the research gaps and future research directions.},
  keywords = {intelligent transportation systems;learning (artificial intelligence);mobile robots;object detection;robot vision;vehicles;3D object detection;autonomous driving applications;autonomous vehicle;AV;perception system;2D detection methods;detection accuracy;fusion methods;point cloud;machine learning;sensory data;Sensors;Three-dimensional displays;Object detection;Cameras;Laser radar;Autonomous vehicles;Two dimensional displays;Machine learning;deep learning;computer vision;object detection;autonomous vehicles;intelligent vehicles},
}

@InProceedings{Xiang2015,
  author    = {Y. {Xiang} and {Wongun Choi} and Y. {Lin} and S. {Savarese}},
  title     = {Data-driven 3D Voxel Patterns for object category recognition},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  pages     = {1903-1911},
}

@InProceedings{Chabot2017,
  author    = {F. {Chabot} and M. {Chaouch} and J. {Rabarisoa} and C. {Teulière} and T. {Chateau}},
  title     = {Deep MANTA: A Coarse-to-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis from Monocular Image},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  pages     = {1827-1836},
}

@InProceedings{Chen2016,
  author    = {X. {Chen} and K. {Kundu} and Z. {Zhang} and H. {Ma} and S. {Fidler} and R. {Urtasun}},
  title     = {Monocular 3D Object Detection for Autonomous Driving},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {2147-2156},
}

@InProceedings{Li2017,
  author    = {T. {Li} and X. {Zhao}},
  title     = {Cost efficient subcategory-aware CNN for object detection},
  booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
  year      = {2017},
  pages     = {4202-4206},
}

@InProceedings{Shridhar2018,
  author    = {K. Shridhar and F. Laumann and A. L. Maurin and Martin Olsen and Marcus Liwicki},
  title     = {Bayesian Convolutional Neural Networks with Variational Inference},
  booktitle = {Master Thesis},
  year      = {2018},
}

@Article{Mukhoti2018,
  author  = {Jishnu Mukhoti and Yarin Gal},
  title   = {Evaluating Bayesian Deep Learning Methods for Semantic Segmentation},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1811.12709},
}

@InProceedings{Tran2019,
  author    = {Dustin Tran and Michael W. Dusenberry and M. V. D. Wilk and Danijar Hafner},
  title     = {Bayesian Layers: A Module for Neural Network Uncertainty},
  booktitle = {NeurIPS},
  year      = {2019},
}

@Article{Gurau2018,
  author  = {C. Gurau and A. Bewley and I. Posner},
  title   = {Dropout Distillation for Efficiently Estimating Model Confidence},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1809.10562},
}

@Article{Wen2018,
  author  = {Yeming Wen and Paul Vicol and Jimmy Ba and Dustin Tran and Roger B. Grosse},
  title   = {Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1803.04386},
}

@InProceedings{Ovadia2019,
  author    = {Y. Ovadia and E. Fertig and J. Ren and Zachary Nado and D. Sculley and Sebastian Nowozin and Joshua V. Dillon and Balaji Lakshminarayanan and Jasper Snoek},
  title     = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
  booktitle = {NeurIPS},
  year      = {2019},
}

@InCollection{Graves2011,
  author    = {Graves, Alex},
  title     = {Practical Variational Inference for Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {Curran Associates, Inc.},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {2348--2356},
}

@InCollection{Jaderberg2015,
  author    = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
  title     = {Spatial Transformer Networks},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {2017--2025},
}

@Article{Dillon2017,
  author  = {Joshua V. Dillon and I. Langmore and Dustin Tran and E. Brevdo and Srinivas Vasudevan and David A. Moore and B. Patton and Alexander Amir Alemi and M. Hoffman and R. A. Saurous},
  title   = {TensorFlow Distributions},
  journal = {ArXiv},
  year    = {2017},
  volume  = {abs/1711.10604},
}

@InCollection{Kingma2015,
  author    = {Kingma, Durk P and Salimans, Tim and Welling, Max},
  title     = {Variational Dropout and the Local Reparameterization Trick},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {2575--2583},
}

@InProceedings{Caesar2020,
  author    = {H. {Caesar} and V. {Bankiti} and A. H. {Lang} and S. {Vora} and V. E. {Liong} and Q. {Xu} and A. {Krishnan} and Y. {Pan} and G. {Baldan} and O. {Beijbom}},
  title     = {nuScenes: A Multimodal Dataset for Autonomous Driving},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020},
  pages     = {11618-11628},
}

@InProceedings{Mousavian2017,
  author    = {A. {Mousavian} and D. {Anguelov} and J. {Flynn} and J. {Košecká}},
  title     = {3D Bounding Box Estimation Using Deep Learning and Geometry},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  pages     = {5632-5640},
  month     = {July},
  abstract  = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
  issn      = {1063-6919},
  keywords  = {feature extraction;image segmentation;learning (artificial intelligence);neural nets;object detection;object recognition;pose estimation;regression analysis;3D bounding box estimation;deep learning;deep convolutional neural network;geometric constraints;2D bounding box;3D bounding boxes;3D viewpoint estimation;Pascal 3D;object detection;hybrid discrete-continuous loss;semantic segmentation;instance level segmentation;flat ground priors;Three-dimensional displays;Two dimensional displays;Solid modeling;Pose estimation;Object detection;Shape},
}

@InCollection{Chen2015,
  author    = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew G and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  title     = {3D Object Proposals for Accurate Object Class Detection},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {424--432},
}

@Article{Pham2017,
  author     = {Pham, Cuong Cao and Jeon, Jae Wook},
  title      = {Robust Object Proposals Re-Ranking for Object Detection in Autonomous Driving Using Convolutional Neural Networks},
  journal    = {Image Commun.},
  year       = {2017},
  volume     = {53},
  number     = {C},
  pages      = {110–122},
  month      = apr,
  issn       = {0923-5965},
  abstract   = {Object proposals have recently emerged as an essential cornerstone for object detection. The current state-of-the-art object detectors employ object proposals to detect objects within a modest set of candidate bounding box proposals instead of exhaustively searching across an image using the sliding window approach. However, achieving high recall and good localization with few proposals is still a challenging problem. The challenge becomes even more difficult in the context of autonomous driving, in which small objects, occlusion, shadows, and reflections usually occur. In this paper, we present a robust object proposals re-ranking algorithm that effectivity re-ranks candidates generated from a customized class-independent 3DOP (3D Object Proposals) method using a two-stream convolutional neural network (CNN). The goal is to ensure that those proposals that accurately cover the desired objects are amongst the few top-ranked candidates. The proposed algorithm, which we call DeepStereoOP, exploits not only RGB images as in the conventional CNN architecture, but also depth features including disparity map and distance to the ground. Experiments show that the proposed algorithm outperforms all existing object proposal algorithms on the challenging KITTI benchmark in terms of both recall and localization. Furthermore, the combination of DeepStereoOP and Fast R-CNN achieves one of the best detection results of all three KITTI object classes. HighlightsWe present a robust object proposals re-ranking algorithm for object detection in autonomous driving.Both RGB images and depth features are included in the proposed two-stream CNN architecture called DeepStereoOP.Initial object proposals are generated from a customized class-independent 3DOP method.Experiments show that the proposed algorithm outperforms all existing object proposals algorithms.The combination of DeepStereoOP and Fast R-CNN achieves one of the best detection results on KITTI benchmark.},
  address    = {USA},
  issue_date = {April 2017},
  keywords   = {Stereo vision., Convolutional neural networks, Autonomous driving, Object proposals, Object detection},
  numpages   = {13},
  publisher  = {Elsevier Science Inc.},
}

@InProceedings{Zhou2018,
  author    = {Y. {Zhou} and O. {Tuzel}},
  title     = {VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2018},
  pages     = {4490-4499},
  month     = {June},
  abstract  = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  issn      = {2575-7075},
  keywords  = {feature extraction;image coding;image representation;learning (artificial intelligence);object detection;optical radar;radar imaging;virtual reality;detections;KITTI car detection benchmark show;3D detection methods;end-to-end learning;3D object detection;3D point clouds;highly sparse LiDAR point cloud;region proposal network;hand-crafted feature representations;manual feature engineering;generic 3D detection network;feature extraction;bounding box prediction;end-to-end trainable deep network;transforms;unified feature representation;VoxelNet;voxel feature;Three-dimensional displays;Laser radar;Feature extraction;Shape;Proposals;Encoding},
}

@InProceedings{Sahba2019,
  author    = {R. {Sahba} and A. {Sahba} and M. {Jamshidi} and P. {Rad}},
  title     = {3D Object Detection Based on LiDAR Data},
  booktitle = {2019 IEEE 10th Annual Ubiquitous Computing, Electronics Mobile Communication Conference (UEMCON)},
  year      = {2019},
  pages     = {0511-0514},
  month     = {Oct},
  abstract  = {Object detection has been a very hot research topic since the advent of artificial intelligence and machine learning. Its importance is very high specifically in advancing autonomous vehicles technology. Many object detection methods have been developed based on different types of data including image, radar, and lidar. Some recent works use point clouds for 3D object detection. One of the recently presented efficient methods is PointPillars, an encoder which learns from data in a point cloud and organizes a representation in vertical columns (pillars) for 3D object detection. in this work, we use PointPillars with lidar data of some urban scenes provided in nuScenes dataset to predict 3D boxes for three different classes of objects (car, pedestrian, bus). We also use nuScenes detection score (NDS) which is a consolidated metric for detection task, to measure and compare different scenarios. Results show that by increasing the number of lidar sweeps, the performance of the 3D object detector improves significantly. We try to increase the performance of the encoder by developing a method to combine different types of input data (lidar, radar, image) based on a weighting system and use it as the input of the encoder.},
  keywords  = {learning (artificial intelligence);object detection;optical radar;radar computing;radar imaging;lidar data;artificial intelligence;machine learning;nuScenes detection score;3D object detector;autonomous vehicles technology;PointPillars;3D Object Detection;Encoder;Lidar;Dataset;Point Cloud},
}

@Article{Simon2018,
  author  = {M. Simon and S. Milz and Karl Amende and H. Gro{\ss}},
  title   = {Complex-YOLO: Real-time 3D Object Detection on Point Clouds},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1803.06199},
}

@Article{Du2018,
  author  = {Xinxin Du and M. Ang and Sertac Karaman and D. Rus},
  title   = {A General Pipeline for 3D Detection of Vehicles},
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year    = {2018},
  pages   = {3194-3200},
}

@Book{Kullback1959,
  title     = {{Information Theory and Statistics}},
  publisher = {Dover Publications},
  year      = {1959},
  author    = {Solomon Kullback and Richard Leibler},
  editor    = {{}},
}

@Article{WBAW1991,
  author  = {{Wray L. Buntine and Andreas S. Weigend}},
  title   = {{Bayesian Back-Propagation}},
  journal = {{Complex Systems}},
  year    = {1991},
  volume  = {5},
  pages   = {603},
}

@InProceedings{Hinton1993,
  author    = {Hinton, Geoffrey E. and van Camp, Drew},
  title     = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
  booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory},
  year      = {1993},
  series    = {COLT '93},
  pages     = {5–13},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  isbn      = {0897916115},
  location  = {Santa Cruz, California, USA},
  numpages  = {9},
}

@Misc{ShuyangCheng2020,
  author       = {{Shuyang Cheng, Yang Song and Peisheng Li}},
  title        = {{Using automated data augmentation to advance our Waymo Driver}},
  howpublished = {Technical Blog},
  month        = apr,
  year         = {2020},
  timestamp    = {2020-02-24},
  url          = {https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html},
}

@Article{Ge2020,
  author   = {Yuanyue Ge and Ya Xiong and Pål J. From},
  title    = {Symmetry-based 3D shape completion for fruit localisation for harvesting robots},
  journal  = {Biosystems Engineering},
  year     = {2020},
  volume   = {197},
  pages    = {188 - 202},
  issn     = {1537-5110},
  abstract = {Fruit localisation is a crucial step in developing a robotic fruit-harvesting system. This paper aims to improve the localisation accuracy of fruits in 3D space. In the machine vision system of a harvesting robot, in a single view the visible area of a target is often incomplete and therefore, cannot be directly used to accurately determine the target location. A 3D shape completion method is proposed that can be used on the partially visible images of strawberries obtained from a single view. This method proposed a given number of symmetric plane candidates based on the assumption that the targets are symmetrical, which is normally true for fruits such as such apples, citrus fruits and strawberries. Corresponding rating rules were proposed to select the optimal symmetry to be used for the shape completion. The algorithm was then tested on reconstructed point clouds and implemented on a strawberry harvester equipped with a Red Green Blue-Depth (RGB-D) camera. The evaluation on reconstructed strawberry data showed that the intersection over union (IoU) and centre deviation between the results obtained by this method and ground truth were 0.77 and 6.9 mm, respectively, whilst those of the unprocessed partial data were 0.56 and 14.1 mm. The evaluation results of the strawberry data captured with the RGB-D camera showed that the IoU and centre deviation between the results obtained by this method and ground truth were 0.61 and 5.7 mm, respectively, whilst those of the unprocessed partial data were 0.47 and 8.9 mm.},
  keywords = {Strawberry harvesting, Machine vision, Localisation, Shape completion},
}

@Proceedings{Depeweg2017,
  title         = {Uncertainty Decomposition in Bayesian Neural Networks with Latent Variables},
  year          = {2017},
  __markedentry = {[jaswa:]},
  author        = {Depeweg, Stefan and Jose Miguel Hernandez-Lobato and Finale Doshi-Velez and Steffen Udluft},
  journal       = {International Conference on Machine Learning (ICML) Workshop},
}

@InProceedings{Feng2018,
  author        = {D. {Feng} and L. {Rosenbaum} and K. {Dietmayer}},
  title         = {Towards Safe Autonomous Driving: Capture Uncertainty in the Deep Neural Network For Lidar 3D Vehicle Detection},
  booktitle     = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  year          = {2018},
  pages         = {3266-3273},
  month         = {Nov},
  __markedentry = {[jaswa:6]},
}

@InProceedings{valdenegrotoro2019deep,
  author    = {{Matias Valdenegro-Toro}},
  title     = {{Deep Sub-Ensembles for Fast Uncertainty Estimation in Image Classification}},
  booktitle = {{Bayesian Deep Learning Workshop. Neural Information Processing Systems (NIPS-2019), befindet sich NeurIPS 2019, December 8-14, Vancouver, BC, Canada}},
  year      = {2019},
  month     = dec,
}

@Comment{jabref-meta: databaseType:bibtex;}
